{"posts":[{"title":"用Java实现SQL的嵌套集设计 - 先序树遍历","text":"English 前言 嵌套集模型 使用Java实现 MySQL表设计 构建映射实体 数据编号入库 根据业务需求，查询数据 扩展查询语句 Note 前言最近有一个目录文件入库的需求，条件是： 入参是解压后的文件夹路径； 解压后的文件不存在变更及更新的情况。 需求详情是： 文件夹及其所有子目录和子文件，都需要解析成树结构响应给前端； 需要在点击每一级目录时，都拿到这个目录下所有文件（包括子目录下的文件）进行一些业务数据的统计； 只能使用关系型数据库MySQL。 经过调研， 发现嵌套集设计 (译文1) (译文2) 很适合这样的场景。Note - 如果项目中用到或可以用图数据库，图数据库是处理复杂层次数据更好的选择。 互联网上很难找到使用Java实现的现成代码，只在github上找到一个项目实现了一部分功能。但此项目不支持MyBatis，我不计划采用，因此只能自己实现嵌套集设计。 这篇文章会简单介绍嵌套集模型 Nested Set Model，详细介绍如何使用Java，从建表到入库实现嵌套集模型。 本文示例使用 MySQL 8.0、JDK 8 嵌套集模型在面对分层结构数据存储时，例如目录， 我们往往采用被称为邻接表模型(Adjacency List)的方案，表字段设计大约是： 1id, name, parentId 在邻接表中，所有的数据均拥有一个parentId字段，用来存储它的父节点ID。当前节点为根节点的话，它的父节点则为NULL或者-1。在遍历时，可以使用递归实现查询整棵树，也可以方便地查询到下一级节点。增删也方便。但是当数据量较大时，查询整棵树会影响性能，甚至导致内存溢出。因此在使用中，我们通常会使用懒加载的方式，一级一级展示数据。或者限制递归的深度，只展示一部分数据。 如果已知数据增删改很少，对查询性能要求比较高、并且像我们的需求一样，需要查某节点下所有叶子节点，就可以考虑使用嵌套集模型了。嵌套集模型的设计大约是： 1id, name, left_index, right_index, depth 也就是把各个节点看做一个个容器，子节点在父节点内部，所有节点都在根节点中；用图片表示如下： 再自左向右编号，每个容器都有左右两个编号，即为left与right; 用图片表示如下： 为了方便查询下一级节点和其他节点，可以增加一个depth字段，用来表示深度。至此，可以得到每个节点的左右值。下文仅列举几个最常用的SQL，其他SQL可在嵌套集设计中查看. 使用Java实现MySQL表设计123456789101112CREATE TABLE `file_nested_sets_demo` ( `id` varchar(100) NOT NULL COMMENT '文件ID，唯一标识', `path` varchar(2000) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '文件完整路径', `type` varchar(100) DEFAULT NULL COMMENT '文件类型; Directory,File', `size` double DEFAULT NULL COMMENT '文件大小', `tree_id` bigint DEFAULT NULL COMMENT '每棵树的ID', `left_index` bigint NOT NULL COMMENT '左值', `right_index` bigint NOT NULL COMMENT '右值', `depth` bigint NOT NULL COMMENT '深度', PRIMARY KEY (`id`), KEY `idx_tree_id_indexes` (`tree_id`,`left_index`,`right_index`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='文件嵌套集表示例'; 构建映射实体123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 入库使用的嵌套集对象 */public class FileNestedSetsDemo { /** * 文件ID，唯一标识 */ private String id; /** * 文件完整路径 */ private String path; /** * 文件类型； FILE/DIRECTORY */ private String type; /** * 文件大小 */ private Double size; /** * 树ID */ private Long treeId; /** * 左值 */ private Long leftIndex; /** * 右值 */ private Long rightIndex; /** * 深度 */ private Long depth;} 数据编号入库已知输入的数据为文件路径，路径结构同上图，如： 1234567891011|== 表示文件夹； |-- 表示文件。 |==resources |==data |--table_design |==mapper |==server |--ServerMapper.xml |==source |--AMapper.xml |--bootstrap.properties |--logback-spring.xml 需要的输出的数据是： 1234567891011depth left_index|||path|||right_index0 1|||\\resources|||201 2|||\\resources\\data||52 3|||\\resources\\data\\table_design|||41 6|||\\resources\\mapper|||152 7|||\\resources\\mapper\\server|||103 8|||\\resources\\mapper\\server\\ServerMapper.xml|||92 11|||\\resources\\mapper\\source|||122 13|||\\resources\\mapper\\AMapper.xml|||141 16|||\\resources\\bootstrap.properties|||171 18|||\\resources\\logback-spring.xml|||19 我们可采用先序遍历算法构造数据，从左到右、一次一层，遍历其子节点并赋值。以下代码使用深度优先的方式，对目录进行遍历赋值，返回Map。该算法可能不够完备，如果有任何优化意见，请在评论区分享。 NestedSetsUtil >folded1...代码块内容... 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class NestedSetsUtil { /** * 使用深度优先遍历目录，返回Map，用来支持嵌套集初次入库。 * * @param root 需要遍历的目录 * @return Map&lt;file对象，Object对象&gt; **/ public static Map&lt;File, NestedSetObj&gt; dfs2NestedSets(File root) { if (root == null) { return new HashMap&lt;&gt;(0); } //记录深度 每次出栈-1, 每次入栈+1 long depth = 0L; //左右值 long index = 1L; Deque&lt;File&gt; stack = new ArrayDeque&lt;&gt;(); stack.push(root); //全局的对象集合，同时也表示了这个对象是否入栈过。 Map&lt;File, NestedSetObj&gt; map = new LinkedHashMap&lt;&gt;(); //根目录，直接给值 NestedSetObj rootObj = NestedSetObj.builder().path(root.getAbsolutePath()).depth(depth).left(index++).build(); map.put(root, rootObj); while (!stack.isEmpty()) { File cur = stack.pop(); depth--; File[] files = cur.listFiles(); if (files != null) { //找到下一个节点就执行 for (File next : files) { //下一个节点如果没有被访问过就执行 if (!map.containsKey(next)) { //当前节点和下一个节点入栈 depth++; stack.push(cur); depth++; stack.push(next); //每首次访问，节点赋值left NestedSetObj obj = NestedSetObj.builder().left(index++).path(next.getAbsolutePath()).depth(depth).build(); boolean leaf = !next.isDirectory(); boolean emptyDirLeaf = next.listFiles() != null &amp;&amp; next.listFiles().length == 0; //叶子节点或空目录直接赋right值 if (leaf || emptyDirLeaf) { obj.setRight(index++); } map.put(next, obj); break; } } //判断是否需要给叶子节点的right赋值；我们认为，只有当前目录下，所有子一级的数据都有了right值，则当前目录可以被赋值。 long min = Long.MAX_VALUE; for (File file : files) { NestedSetObj childObject = map.get(file); if (childObject == null) { min = 0L; break; } min = Math.min(min, childObject.getRight()); } //给非叶子节点的right赋值;当一级子节点都有了right值，index 加一即为当前节点的right值。 if (min &gt; 0L &amp;&amp; Long.MAX_VALUE != min) { NestedSetObj curObj = map.get(cur); if (curObj.right == 0L) { curObj.setRight(index++); } } } } //根目录的右值=节点数量*2 rootObj.setRight(map.size() * 2L); return map; } /** * 文件嵌套集对象，业务无关 */ public static class NestedSetObj { private long left; private long right; private long depth; private String path; }} 接着把 NestedSetsUtil#dfs2NestedSets() 响应的数据遍历，赋值给FileNestedSetsDemo，批量入库即可。 根据业务需求，查询数据 检索单个路径下的所有类型为文件的子节点 12345678910SELECT node.*FROM file_nested_sets_demo AS node, file_nested_sets_demo AS parentWHERE node.depth = parent.depth + 1 AND node.left_index &gt; parent.left_index AND node.right_index &lt; parent.right_index AND node.tree_id = 1 AND parent.path = '\\resources\\mapper' AND node.type = 'FILE'ORDER BY parent.left_index 检索单个树下的所有子节点 1234SELECT node.*FROM file_nested_sets_demo AS nodeWHERE node.tree_id = 1ORDER BY node.left_index 扩展查询语句 检索单个路径下的直接子节点123456789SELECT node.*FROM file_nested_sets_demo AS node, file_nested_sets_demo AS parentWHERE node.depth = parent.depth + 1 AND node.left_index &gt; parent.left_index AND node.right_index &lt; parent.right_index AND node.tree_id = 1 AND parent.path = '\\resources\\mapper'ORDER BY parent.left_index Note可以执行的Java算法见 github","link":"/zh-CN/implement-SQL-nested-sets-in-Java-zh/"},{"title":"单节点 ElasticSearch 及 Kibana 安装说明","text":"English 概述 安装ES 安装Kibana 配置权限(使用用户名和密码身份验证运行本地集群) 附录 概述为了支持新功能，我们新增了ES节点。 可以根据数据情况和状态，配置ES为单节点或集群; 开启Xpack, 启用权限认证(需要安装Kibana)。官方文档 Set up Elasticsearch 有各个 OS 的安装指导，Installing Elasticsearch 页面中提供了多种安装包对应的指导链接，可以参考。本文档为在单节点linux服务器上安装ES及Kibana的说明文档。 安装ES 首先确认环境中有JDK。 Elasticsearch 7.x 包里自包含了 OpenJDK11 的包，如果需要用自己的版本，参考官方文档设置 JAVA_HOME 环境变量。 创建ES专用用户，因为无法使用root用户启动： 1useradd elasticsearch 创建ES程序目录，并给elasticsearch用户赋权限： 123cd /homemkdir /eschown -R elasticsearch:elasticsearch /home/es/ 上传压缩包内的elasticsearch-7.14.0-linux-x86_64.tar.gz，也可去官网下载或使用其他方式下载。官网下载地址, 国内镜像下载地址 解压 1tar -zxvf elasticsearch-7.14.0-linux-x86_64.tar.gz 修改配置文件，进入解压后的目录 1cd elasticsearch-7.14.0/config 首先备份配置文件elasticsearch.yml，而后修改 12345678cp elasticsearch.yml elasticsearch.yml.bak vim elasticsearch.yml------------------------------network.host: ${该服务器的ip}http.port: 9200discovery.seed_hosts: [&quot;${该服务器的ip}&quot;]discovery.type: single-node # 单节点模式------------------------------ 把9200和9300端口开放，或者关闭防火墙 根据配置文件，创建data目录存储es数据 1mkdir data 给ES用户所有ES相关的权限；切换到elasticsearch用户；在bin目录下启动ES 1234chown -R elasticsearch:elasticsearch ./*su – elasticsearchcd /home/es/elasticsearch-7.14.0/bin./elasticsearch &amp; 启动后可能会出现报错： 12trying to update state on non-existing task geoip-downloader[2021-08-11T15:33:57,318][ERROR][o.e.i.g.GeoIpDownloader ] [18789989a729] error updating geoip database [GeoLite2-Country.mmdb] 该报错可以忽略；若想解决请看附录 1.解决ES启动报错问题 验证启动情况，在本机执行 1curl http://${ip}:9200/ 得到返回结果如: 1234567891011121314151617{ &quot;name&quot; : &quot;localhost.localdomain&quot;, &quot;cluster_name&quot; : &quot;oss-es&quot;, &quot;cluster_uuid&quot; : &quot;HKCnF4l_TOSW8-mznxM5eg&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;7.14.0&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1&quot;, &quot;build_date&quot; : &quot;2021-07-29T20:49:32.864135063Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.9.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 即为成功！ 再尝试通过浏览器访问http://${ip}:9200/有相同的响应成功结果。 若无法访问，则检查防火墙。 安装Kibana 安装步骤可参考官方网站；或按以下步骤执行 使用root用户，上传压缩包内的kibana-7.14.0-linux-x86_64.tar.gz并解压。 1tar -zxvf kibana-7.14.0-linux-x86_64.tar.gz 进入目录备份配置文件kibana.yml，而后修改 123456789cd kibana-7.14.0-linux-x86_64/config/cp kibana.yml kibana.yml.bakvim kibana.yml ------------------------------server.port: 5601server.host: &quot;${该服务器的ip}&quot;elasticsearch.hosts: [&quot;http://${ES服务所在IP}:9200&quot;]elasticsearch.username: &quot;kibana_system&quot;------------------------------ 进入/bin 目录启动 12cd ../bin./kibana 界面会打印日志，最后出现如下所示内容，即为成功！ 1Kibana is now available 可通过浏览器访问 http://ip:5601 配置权限(使用用户名和密码身份验证运行本地集群) 可以参考官网最低安全性设置 停止 Kibana 和 Elasticsearch（如果它们正在运行） 将 xpack.security.enabled 设置添加到**$ES_PATH_CONF/elasticsearch.yml**文件并将值设置为true 12345 cd /home/es/elasticsearch-7.14.0/config vim elasticsearch.yml-----添加以下信息-------xpack.security.enabled:true---------------------- tips:该$ES_PATH_CONF变量是 Elasticsearch 配置文件的路径。如果您使用存档分发版（zip或tar.gz）安装了 Elasticsearch ，则该变量默认为$ES_HOME/config. 如果您使用软件包发行版（Debian 或 RPM），则该变量默认为/etc/elasticsearch. 启动ES，等待启动成功 12cd ../bin./elasticsearch 打开另一个终端窗口，进入ES目录，执行命令启用配置密码工具 1./bin/elasticsearch-setup-passwords interactive 执行后根据命令行提示配置密码 配置 Kibana 以使用密码连接到 Elasticsearch ，创建 Kibana 密钥库并添加安全设置： 123cd kibana-7.14.0-linux-x86_64/ ./bin/kibana-keystore create ./bin/kibana-keystore add elasticsearch.password 出现提示时，输入kibana_system用户的密码。 重启 Kibana,并在浏览器 “http://${kibanaIp}:5601” 以elastic用户身份登录 Kibana 。 1./bin/kibana 附录 解决ES启动报错问题 参考 How to disable geoip usage in 7.14.0方案：使用 cluster settings API 而不是 elasticsearch.yml; 即安装好Kibana后，执行123456PUT _cluster/settings{ &quot;persistent&quot;: { &quot;ingest.geoip.downloader.enabled&quot;: false }}","link":"/zh-CN/single-node-elasticsearch-and-kibana-installation-instructions-zh/"}],"tags":[{"name":"SQL","slug":"SQL","link":"/zh-CN/tags/SQL/"},{"name":"Java","slug":"Java","link":"/zh-CN/tags/Java/"},{"name":"树遍历","slug":"树遍历","link":"/zh-CN/tags/%E6%A0%91%E9%81%8D%E5%8E%86/"},{"name":"项目经验","slug":"项目经验","link":"/zh-CN/tags/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C/"},{"name":"系统设计","slug":"系统设计","link":"/zh-CN/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/zh-CN/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","link":"/zh-CN/tags/Kibana/"},{"name":"集群","slug":"集群","link":"/zh-CN/tags/%E9%9B%86%E7%BE%A4/"},{"name":"运维","slug":"运维","link":"/zh-CN/tags/%E8%BF%90%E7%BB%B4/"},{"name":"部署","slug":"部署","link":"/zh-CN/tags/%E9%83%A8%E7%BD%B2/"}],"categories":[{"name":"技术","slug":"技术","link":"/zh-CN/categories/%E6%8A%80%E6%9C%AF/"},{"name":"后端开发","slug":"技术/后端开发","link":"/zh-CN/categories/%E6%8A%80%E6%9C%AF/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"运维","slug":"技术/运维","link":"/zh-CN/categories/%E6%8A%80%E6%9C%AF/%E8%BF%90%E7%BB%B4/"},{"name":"项目经验","slug":"技术/项目经验","link":"/zh-CN/categories/%E6%8A%80%E6%9C%AF/%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C/"},{"name":"系统设计","slug":"技术/系统设计","link":"/zh-CN/categories/%E6%8A%80%E6%9C%AF/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"}],"pages":[{"title":"关于","text":"English 生活问题的解答要随着这问题的消失而现身。这不就是之所以如此——在长久的怀疑之后明白了生活的意义的人却说不出这意义在于什么——的原因吗? 路德维希·维特根斯坦逻辑哲学论 BoBoTheKnight欢迎来到BoBoTheKnight 的网站！ 我往往因为好奇而学习和思考。尽管很多问题的答案难以用言语表示，但我还是想分享我的困惑，以及我为了这些困惑进行学习和思考的过程，从而记录我的成长。 更进一步，也许它们可以帮助更多有同样困惑的人。 当然，我也会在这里分享一些技术文章。ps: 即使我自称为骑士，我也仍希望世界和平。 致谢我要感谢一些人和组织，没有他们建立这个网站会很困难。感谢 GitHub 的代码托管服务！感谢 Hexo 的博客项目！感谢 Icarus 提供的漂亮主题！感谢 FontAwesome 提供的精美图标！感谢 FlatIcon 提供的精美图标，特别感谢以下才华横溢的创作者，他们是： 骑士 与 和平 图标的创作者 Freepik ！ LGBT 图标的创作者 Trazobanana！ 和平 图标的创作者 Rashad！","link":"/zh-CN/about/index.html"},{"title":"我们的评论政策","text":"English 欢迎来到 BoboTheKnight 的网站！ 我们重视开放且有意义的讨论，并鼓励您对我们的技术和生活方式文章发表评论。 为了确保为所有访客提供积极且受尊重的环境，我们恳请您遵守以下评论政策： 尊重：以善意、尊重和礼貌对待他人。 避免人身攻击、仇恨言论或任何形式的歧视。 欢迎不同意见，但请以文明和建设性的方式表达您的意见。 紧扣主题：让您的评论与帖子或讨论的内容相关。 避免偏离主题或发布不相关的促销内容。 这有助于保持重点突出且有意义的对话。 禁止垃圾邮件：请勿将评论部分用于自我推销或广告目的。 我们希望确保这些评论有助于讨论，而不仅仅是促销性质的。 避免攻击性或不当内容：确保您的评论不包含露骨、淫秽或攻击性语言或内容。 让我们为每个人维护一个相互尊重和家庭友好的环境。 审核和删除：网站所有者保留审核和删除任何违反此评论政策的评论的权利。 这包括垃圾评论、攻击性评论、偏离主题或其他不当评论。 我们的目标是为每个参与者创造一个积极和建设性的空间。 感谢您花时间阅读并理解我们的评论政策，以及您对 BoboTheKnight 网站社区的参与和贡献。 让我们进行深思熟虑的讨论，分享见解，互相学习，同时保持尊重和包容的氛围。 希望您在这里能度过愉快时光！","link":"/zh-CN/comment-policy/index.html"}]}