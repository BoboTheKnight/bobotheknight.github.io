{"posts":[{"title":"Single Node ElasticSearch and Kibana Installation Instructions","text":"简体中文 Summary Setup ES 安装Kibana 配置权限(使用用户名和密码身份验证运行本地集群) 附录 SummaryTo support the new features, we have added ES nodes. You can configure ES as a single node or cluster depending on the data situation and status; enable Xpack, enable permission authentication (requires Kibana installation).The official documentation Set up Elasticsearch has installation instructions for each OS, Installing Elasticsearch page provides links to instructions for various installation packages, which can be consulted. This document provides instructions for installing ES and Kibana on a single-node Linux server. Setup ES First make sure you have a JDK in your environment. Elasticsearch 7.x package includes the OpenJDK11 package, if you need to use your own version, refer to the official documentation to set the JAVA_HOME environment variable. Create an ES-specific user, as it is not possible to start with root user. 1useradd elasticsearch Create the ES directory and grant permissions to the elasticsearch user. 123cd /homemkdir /eschown -R elasticsearch:elasticsearch /home/es/ Upload elasticsearch-7.14.0-linux-x86_64.tar.gz in the zip archive, or you can go to the official website to download it or use other methods to download it. Unpacked 1tar -zxvf elasticsearch-7.14.0-linux-x86_64.tar.gz Modify the configuration file and go to the unpacked directory 1cd elasticsearch-7.14.0/config First backup the configuration file elasticsearch.yml, and then modify 12345678cp elasticsearch.yml elasticsearch.yml.bak vim elasticsearch.yml------------------------------network.host: ${ip of this server}http.port: 9200discovery.seed_hosts: [&quot;${ip of this server}&quot;]discovery.type: single-node # single-node mode------------------------------ Open ports of 9200 and 9300, or turn off the firewall. 根据配置文件，创建data目录存储es数据 1mkdir data 给ES用户所有ES相关的权限；切换到elasticsearch用户；在bin目录下启动ES 1234chown -R elasticsearch:elasticsearch ./*su – elasticsearchcd /home/es/elasticsearch-7.14.0/bin./elasticsearch &amp; 启动后可能会出现报错： 12trying to update state on non-existing task geoip-downloader[2021-08-11T15:33:57,318][ERROR][o.e.i.g.GeoIpDownloader ] [18789989a729] error updating geoip database [GeoLite2-Country.mmdb] 该报错可以忽略；若想解决请看附录 1.解决ES启动报错问题 验证启动情况，在本机执行 1curl http://${ip}:9200/ 得到返回结果如: 1234567891011121314151617{ &quot;name&quot; : &quot;localhost.localdomain&quot;, &quot;cluster_name&quot; : &quot;oss-es&quot;, &quot;cluster_uuid&quot; : &quot;HKCnF4l_TOSW8-mznxM5eg&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;7.14.0&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1&quot;, &quot;build_date&quot; : &quot;2021-07-29T20:49:32.864135063Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.9.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 即为成功！ 再尝试通过浏览器访问http://${ip}:9200/有相同的响应成功结果。 若无法访问，则检查防火墙。 安装Kibana 安装步骤可参考官方网站；或按以下步骤执行 使用root用户，上传压缩包内的kibana-7.14.0-linux-x86_64.tar.gz并解压。 1tar -zxvf kibana-7.14.0-linux-x86_64.tar.gz 进入目录备份配置文件kibana.yml，而后修改 123456789cd kibana-7.14.0-linux-x86_64/config/cp kibana.yml kibana.yml.bakvim kibana.yml ------------------------------server.port: 5601server.host: &quot;${该服务器的ip}&quot;elasticsearch.hosts: [&quot;http://${ES服务所在IP}:9200&quot;]elasticsearch.username: &quot;kibana_system&quot;------------------------------ 进入/bin 目录启动 12cd ../bin./kibana 界面会打印日志，最后出现如下所示内容，即为成功！ 1Kibana is now available 可通过浏览器访问 http://ip:5601 配置权限(使用用户名和密码身份验证运行本地集群) 可以参考官网最低安全性设置 停止 Kibana 和 Elasticsearch（如果它们正在运行） 将 xpack.security.enabled 设置添加到$ES_PATH_CONF/elasticsearch.yml文件并将值设置为true 12345 cd /home/es/elasticsearch-7.14.0/config vim elasticsearch.yml-----添加以下信息-------xpack.security.enabled:true---------------------- tips:该$ES_PATH_CONF变量是 Elasticsearch 配置文件的路径。如果您使用存档分发版（zip或tar.gz）安装了 Elasticsearch ，则该变量默认为$ES_HOME/config. 如果您使用软件包发行版（Debian 或 RPM），则该变量默认为/etc/elasticsearch. 启动ES，等待启动成功 12cd ../bin./elasticsearch 打开另一个终端窗口，进入ES目录，执行命令启用配置密码工具 1./bin/elasticsearch-setup-passwords interactive 执行后根据命令行提示配置密码 配置 Kibana 以使用密码连接到 Elasticsearch ，创建 Kibana 密钥库并添加安全设置： 123cd kibana-7.14.0-linux-x86_64/ ./bin/kibana-keystore create ./bin/kibana-keystore add elasticsearch.password 出现提示时，输入kibana_system用户的密码。 重启 Kibana,并在浏览器 “http://${kibanaIp}:5601” 以elastic用户身份登录 Kibana 。 1./bin/kibana 附录 解决ES启动报错问题 参考 How to disable geoip usage in 7.14.0方案：使用 cluster settings API 而不是 elasticsearch.yml; 即安装好Kibana后，执行123456PUT _cluster/settings{ &quot;persistent&quot;: { &quot;ingest.geoip.downloader.enabled&quot;: false }}","link":"/2021/12/02/single-node-elasticsearch-and-kibana-installation-instructions/"},{"title":"Nested Set Modified Preorder Tree Traversal Realizing SQL Nested Set Design in Java","text":"简体中文 PreviousRecently, we got a requirement for storage directories and files，the condition is, Input parameter is the path of the root directory which was unpacked. The files will not change nor update after unpacked. The details of the requirement is, The directories and all its subdirectories and sub-files need to be parsed into a tree structure response to the front end. When the user clicks on each level of directory, we need to get the files in this directory and all its subdirectories for some business data statistics. We can only use the MySQL.(Can’t use a graph database.) 经过调研，发现嵌套集设计很适合这样的场景。（如果项目中用到了图数据库，图数据库是处理复杂层次数据更好的选择。） 互联网上很难找到使用Java实现的现成代码，只在github上找到一个项目实现了一部分功能。但此项目不支持MyBatis，我不计划采用，因此只能自己实现嵌套集设计。 这篇文章会简单介绍嵌套集模型 Nested Set Model，详细介绍如何使用Java，从建表到入库实现嵌套集模型。 本文示例使用 MySQL 8.0、JDK 8 Nested Set Model在面对分层结构数据存储时，例如目录， 我们往往采用被称为邻接表模型的方案，表字段设计大约是：1id, name, parentId在邻接表中，所有的数据均拥有一个parentId字段，用来存储它的父节点ID。当前节点为根节点的话，它的父节点则为NULL或者-1。在遍历时，可以使用递归实现查询整棵树，也可以方便地查询到下一级节点。增删也方便。但是当数据量较大时，查询整棵树会影响性能，甚至导致内存溢出。因此在使用中，我们通常会使用懒加载的方式，一级一级展示数据。或者限制递归的深度，只展示一部分数据。 如果已知数据增删改很少，对查询性能要求比较高、并且像我们的需求一样，需要查某节点下所有叶子节点，就可以考虑使用嵌套集模型了。嵌套集模型的设计大约是：1id, name, left_index, right_index, depth也就是把各个节点看做一个个容器，子节点在父节点内部，所有节点都在根节点中；用图片表示如下： 再自左向右编号，每个容器都有左右两个编号，即为left与right; 用图片表示如下： 为了方便查询下一级节点和其他节点，可以增加一个depth字段，用来表示深度。至此，可以得到每个节点的左右值。下文仅列举几个最常用的SQL，其他SQL可在嵌套集设计中查看. 使用Java实现MySQL表设计123456789101112CREATE TABLE `file_nested_sets_demo` ( `id` varchar(100) NOT NULL COMMENT '文件ID，唯一标识', `path` varchar(2000) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '文件完整路径', `type` varchar(100) DEFAULT NULL COMMENT '文件类型; Directory,File', `size` double DEFAULT NULL COMMENT '文件大小', `tree_id` bigint DEFAULT NULL COMMENT '每棵树的ID', `left_index` bigint NOT NULL COMMENT '左值', `right_index` bigint NOT NULL COMMENT '右值', `depth` bigint NOT NULL COMMENT '深度', PRIMARY KEY (`id`), KEY `idx_tree_id_indexes` (`tree_id`,`left_index`,`right_index`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='文件嵌套集表示例'; 构建映射实体123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 入库使用的嵌套集对象 */public class FileNestedSetsDemo { /** * 文件ID，唯一标识 */ private String id; /** * 文件完整路径 */ private String path; /** * 文件类型； FILE/DIRECTORY */ private String type; /** * 文件大小 */ private Double size; /** * 树ID */ private Long treeId; /** * 左值 */ private Long leftIndex; /** * 右值 */ private Long rightIndex; /** * 深度 */ private Long depth;} 数据编号入库已知输入的数据为文件路径，路径结构同上图，如：1234567891011|== 表示文件夹； |-- 表示文件。 |==resources |==data |--table_design |==mapper |==server |--ServerMapper.xml |==source |--AMapper.xml |--bootstrap.properties |--logback-spring.xml 需要的输出的数据是：1234567891011depth left_index|||path|||right_index0 1|||\\resources|||201 2|||\\resources\\data||52 3|||\\resources\\data\\table_design|||41 6|||\\resources\\mapper|||152 7|||\\resources\\mapper\\server|||103 8|||\\resources\\mapper\\server\\ServerMapper.xml|||92 11|||\\resources\\mapper\\source|||122 13|||\\resources\\mapper\\AMapper.xml|||141 16|||\\resources\\bootstrap.properties|||171 18|||\\resources\\logback-spring.xml|||19 我们可采用先序遍历算法构造数据，从左到右、一次一层，遍历其子节点并赋值。以下代码使用深度优先的方式，对目录进行遍历赋值，返回Map。该算法可能不够完备，如果有哪里可以优化，请联系我。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class NestedSetsUtil { /** * 使用深度优先遍历目录，返回Map，用来支持嵌套集初次入库。 * * @param root 需要遍历的目录 * @return Map&lt;file对象，Object对象&gt; **/ public static Map&lt;File, NestedSetObj&gt; dfs2NestedSets(File root) { if (root == null) { return new HashMap&lt;&gt;(0); } //记录深度 每次出栈-1, 每次入栈+1 long depth = 0L; //左右值 long index = 1L; Deque&lt;File&gt; stack = new ArrayDeque&lt;&gt;(); stack.push(root); //全局的对象集合，同时也表示了这个对象是否入栈过。 Map&lt;File, NestedSetObj&gt; map = new LinkedHashMap&lt;&gt;(); //根目录，直接给值 NestedSetObj rootObj = NestedSetObj.builder().path(root.getAbsolutePath()).depth(depth).left(index++).build(); map.put(root, rootObj); while (!stack.isEmpty()) { File cur = stack.pop(); depth--; File[] files = cur.listFiles(); if (files != null) { //找到下一个节点就执行 for (File next : files) { //下一个节点如果没有被访问过就执行 if (!map.containsKey(next)) { //当前节点和下一个节点入栈 depth++; stack.push(cur); depth++; stack.push(next); //每首次访问，节点赋值left NestedSetObj obj = NestedSetObj.builder().left(index++).path(next.getAbsolutePath()).depth(depth).build(); boolean leaf = !next.isDirectory(); boolean emptyDirLeaf = next.listFiles() != null &amp;&amp; next.listFiles().length == 0; //叶子节点或空目录直接赋right值 if (leaf || emptyDirLeaf) { obj.setRight(index++); } map.put(next, obj); break; } } //判断是否需要给叶子节点的right赋值；我们认为，只有当前目录下，所有子一级的数据都有了right值，则当前目录可以被赋值。 long min = Long.MAX_VALUE; for (File file : files) { NestedSetObj childObject = map.get(file); if (childObject == null) { min = 0L; break; } min = Math.min(min, childObject.getRight()); } //给非叶子节点的right赋值;当一级子节点都有了right值，index 加一即为当前节点的right值。 if (min &gt; 0L &amp;&amp; Long.MAX_VALUE != min) { NestedSetObj curObj = map.get(cur); if (curObj.right == 0L) { curObj.setRight(index++); } } } } //根目录的右值=节点数量*2 rootObj.setRight(map.size() * 2L); return map; } /** * 文件嵌套集对象，业务无关 */ public static class NestedSetObj { private long left; private long right; private long depth; private String path; }}接着把 NestedSetsUtil#dfs2NestedSets() 响应的数据遍历，赋值给FileNestedSetsDemo，批量入库即可。 根据业务需求，查询数据 检索单个路径下的所有类型为文件的子节点 12345678910SELECT node.*FROM file_nested_sets_demo AS node, file_nested_sets_demo AS parentWHERE node.depth = parent.depth + 1 AND node.left_index &gt; parent.left_index AND node.right_index &lt; parent.right_index AND node.tree_id = 1 AND parent.path = '\\resources\\mapper' AND node.type = 'FILE'ORDER BY parent.left_index 检索单个树下的所有子节点 1234SELECT node.*FROM file_nested_sets_demo AS nodeWHERE node.tree_id = 1ORDER BY node.left_index 扩展查询语句 检索单个路径下的直接子节点 123456789SELECT node.*FROM file_nested_sets_demo AS node, file_nested_sets_demo AS parentWHERE node.depth = parent.depth + 1 AND node.left_index &gt; parent.left_index AND node.right_index &lt; parent.right_index AND node.tree_id = 1 AND parent.path = '\\resources\\mapper'ORDER BY parent.left_index Note可以执行的Java算法见 github","link":"/2022/03/28/nested-set-modified-preorder-tree-traversal-realizing-sql-nested-set-design-in-java/"}],"tags":[],"categories":[{"name":"ops","slug":"ops","link":"/categories/ops/"},{"name":"sql_design","slug":"sql-design","link":"/categories/sql-design/"}],"pages":[{"title":"About","text":"简体中文 The solution of the problem of life is seen in the vanishing of this problem.(Is not this the reason why men to whom after long doubting the sense of life became clear, could not then say wherein this sense consisted?) Ludwig WittgensteinTractatus Logico-Philosophicus BoBoTheKnightWelcome to BoBoTheKnight’s site! I learn and think often out of curiosity. Although the answers to many questions are unspeakable, I still want to share my learning and thinking process.To record my growth. If possible, help more people with the same confusion. Of course, I will also share some technical articles in here.BTW, even I named myself knight, I wish world peace. CreditI want to thank a few people without whom I would have had a hard time building this site.Thanks to GitHub for code hosting!Thanks to Hexo for the blog project!Thanks to Icarus for the pretty theme!Thanks to FontAwesome for the icons!Thanks to FlatIcon for the icons, especially thanks to the following talented authors, they are :&nbsp;&nbsp;&nbsp;&nbsp; Freepik for the Knight and the Peace icons!&nbsp;&nbsp;&nbsp;&nbsp; Trazobanana for the LGBT icons!&nbsp;&nbsp;&nbsp;&nbsp; Rashad for the Peace icons!","link":"/about/index.html"}]}