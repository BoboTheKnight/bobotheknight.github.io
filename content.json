{"posts":[{"title":"Single Node ElasticSearch and Kibana Installation Instructions","text":"简体中文 Summary Setup ES Install Kibana Configure permissions (run local cluster using username and password authentication) Appendix SummaryTo support the new features, we have added ES nodes. You can configure ES as a single node or cluster depending on the data situation and status; enable Xpack, enable permission authentication (requires Kibana installation).The official documentation Set up Elasticsearch has installation instructions for each OS, Installing Elasticsearch page provides links to instructions for various installation packages, which can be consulted. This document provides instructions for installing ES and Kibana on a single-node Linux server. Setup ES First make sure you have a JDK in your environment. Elasticsearch 7.x package includes the OpenJDK11 package, if you need to use your own version, refer to the official documentation to set the JAVA_HOME environment variable. Create an ES-specific user, as it is not possible to start with root user. 1useradd elasticsearch Create the ES directory and grant permissions to the elasticsearch user. 123cd /homemkdir /eschown -R elasticsearch:elasticsearch /home/es/ Upload elasticsearch-7.14.0-linux-x86_64.tar.gz in the zip archive, or you can go to the official website to download it or use other methods to download it. Unpacked: 1tar -zxvf elasticsearch-7.14.0-linux-x86_64.tar.gz Modify the configuration file and go to the unpacked directory: 1cd elasticsearch-7.14.0/config First backup the configuration file elasticsearch.yml, and then modify: 12345678cp elasticsearch.yml elasticsearch.yml.bak vim elasticsearch.yml------------------------------network.host: ${ip of this server}http.port: 9200discovery.seed_hosts: [&quot;${ip of this server}&quot;]discovery.type: single-node # single-node mode------------------------------ Open ports of 9200 and 9300, or turn off the firewall. According to the configuration file, create the data directory to store es data. 1mkdir data Give the ES user all ES-related permissions; switch to the elasticsearch user; start ES in the bin directory: 1234chown -R elasticsearch:elasticsearch ./*su – elasticsearchcd /home/es/elasticsearch-7.14.0/bin./elasticsearch &amp; You may get an error after startup: 12trying to update state on non-existing task geoip-downloader[ERROR][o.e.i.g.GeoIpDownloader ] [18789989a729] error updating geoip database [GeoLite2-Country.mmdb] This error can be ignored; to resolve it, see Appendix 1. Resolving ES Startup Error Reporting. Verify startup by running locally: 1curl http://${ip}:9200/ The return result is as follows. 1234567891011121314151617{ &quot;name&quot; : &quot;localhost.localdomain&quot;, &quot;cluster_name&quot; : &quot;oss-es&quot;, &quot;cluster_uuid&quot; : &quot;HKCnF4l_TOSW8-mznxM5eg&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;7.14.0&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1&quot;, &quot;build_date&quot; : &quot;2021-07-29T20:49:32.864135063Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.9.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} That is success!Try again to access http://${ip}:9200/ through your browser with the same response successful result. If you can’t access it, check the firewall. Install Kibana Refer to the official website for the installation procedure; or follow these steps: Using the root user, upload the kibana-7.14.0-linux-x86_64.tar.gz in the zip file and extract it. 1tar -zxvf kibana-7.14.0-linux-x86_64.tar.gz Go to the directory and backup the configuration file kibana.yml and modify this: 123456789cd kibana-7.14.0-linux-x86_64/config/cp kibana.yml kibana.yml.bakvim kibana.yml ------------------------------server.port: 5601server.host: &quot;${ip of this server}&quot;elasticsearch.hosts: [&quot;http://${IP of the ES service}:9200&quot;]elasticsearch.username: &quot;kibana_system&quot;------------------------------ Go to the /bin directory and start, 12cd ../bin./kibana The interface prints a log and is successful when it finally appears as shown below! 1Kibana is now available It can be accessed through a browser http://ip:5601. Configure permissions (run local cluster using username and password authentication) You can refer to the official website Minimum Security Settings. Stop Kibana and Elasticsearch (if they are running). Add the xpack.security.enabled setting to the $ES_PATH_CONF/elasticsearch.yml file and set the value to true. 12345 cd /home/es/elasticsearch-7.14.0/config vim elasticsearch.yml----- add the following information ---- xpack.security.enabled:true---------------------------------------- Tips:This $ES_PATH_CONF variable is the path to the Elasticsearch configuration file. If you installed Elasticsearch using an archive distribution (zip or tar.gz), the variable defaults to $ES_HOME/config. If you are using a package distribution (Debian or RPM), the variable defaults to /etc/elasticsearch.. Start ES and wait for it to start successfully. 12cd ../bin./elasticsearch Open another terminal window, go to the ES directory, and enable the Configuration Password Tool by executing the command 1./bin/elasticsearch-setup-passwords interactive Execute the command and follow the command line prompts to configure passwords. Configure Kibana to connect to Elasticsearch with a password, create a Kibana keystore, and add security settings: 123cd kibana-7.14.0-linux-x86_64/ ./bin/kibana-keystore create ./bin/kibana-keystore add elasticsearch.password When prompted, enter the password for the kibana_system user. Restart Kibana and log in to Kibana as the elastic user in your browser at “http://${kibanaIp}:5601”. 1./bin/kibana Appendix Resolving ES startup errors Refer to How to disable geoip usage in 7.14.0Option: Use cluster settings API instead of elasticsearch.yml; i.e., after installing Kibana, run123456PUT _cluster/settings{ &quot;persistent&quot;: { &quot;ingest.geoip.downloader.enabled&quot;: false }}","link":"/single-node-elasticsearch-and-kibana-installation-instructions-en/"},{"title":"Implementation of nested set design of SQL with Java - Preorder Tree Traversal","text":"简体中文 Preface Nested Set Model Implemented in Java MySQL Table Design Constructing mapping entities Data number entry Query data based on business requirements Extended Query Statements Note PrefaceRecently, we got a requirement for storage directories and files，the condition is, Input parameter is the path of the root directory which was unpacked. The files will not change nor update after unpacked. The details of the requirement is, The directories and all its subdirectories and sub-files need to be parsed into a tree structure response to the front end. When the user clicks on each level of directory, we need to get the files in this directory and all its subdirectories for some business data statistics. We can only use the MySQL.(Can’t use a graph database.) After research, the found that nested set design is well suited for such a scenario.1Note - If graph databases are used in the project, graph databases are a better choice for handling complex hierarchical data. It is hard to find ready-made code on the Internet that uses Java to implement this, and only found a project on github that implements part of the functionality. However, this project does not support MyBatis and I do not plan to adopt it, so I can only implement the nested set design by myself. This article will briefly introduce the Nested Set Model Nested Set Model, details of how to use Java, from the table to the library to implement the nested set model. The examples in this article use MySQL 8.0, JDK 8. Nested Set ModelWhen faced with hierarchically structured data stores, such as catalogs, We tend to use the scheme known as the neighboring table model with table field designs of approx:1id, name, parentIdIn the adjacency table, all data has a parentId field to store its parent node ID. if the current node is the root node, its parent node is NULL or -1. When traversing, you can use recursion to query the whole tree, but also easy to query the next level of nodes. It is also easy to add and delete. However, when the amount of data is large, querying the whole tree will affect performance, and even lead to memory overflow.Therefore, in use, we usually use lazy loading to show the data one level at a time. Or limit the depth of the recursion to show only part of the data. If the known data additions, deletions and changes are rare, the query performance requirements are relatively high, and like our needs, we need to check all the leaf nodes under a node, you can consider using the nested set model. The design of the nested set model is approximately:1id, name, left_index, right_index, depthIn other words, this views the individual nodes as individual containers, with the child nodes inside the parent node and all nodes in the root node; represented in a picture as follows: Then numbered from left to right, each container has a left and right two numbers, that is, for the left and right; represented by the picture as follows: To make it easier to query the next level node and other nodes, a depth field can be added to indicate the depth.At this point, the left and right values of each node can be obtained.Below are just a few of the most commonly used SQLs. Other SQLs can be found in Managing Hierarchical Data in MySQL. Implemented in JavaMySQL Table Design123456789101112CREATE TABLE `file_nested_sets_demo` ( `id` varchar(100) NOT NULL COMMENT 'Document ID，unique identifier', `path` varchar(2000) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT 'Full path of the file', `type` varchar(100) DEFAULT NULL COMMENT 'Document type; Directory,File', `size` double DEFAULT NULL COMMENT 'file size', `tree_id` bigint DEFAULT NULL, `left_index` bigint NOT NULL, `right_index` bigint NOT NULL, `depth` bigint NOT NULL, PRIMARY KEY (`id`), KEY `idx_tree_id_indexes` (`tree_id`,`left_index`,`right_index`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='文件嵌套集表示例'; Constructing mapping entities123456789101112131415161718192021222324252627282930313233/** * Nested set objects used by the deposit database */public class FileNestedSetsDemo { /** * Document ID, unique identification */ private String id; /** * Full path of the file */ private String path; /** * Document type; FILE/DIRECTORY */ private String type; /** * file size */ private Double size; private Long treeId; private Long leftIndex; private Long rightIndex; private Long depth;} Data number entryThe input data is known to be a file path, and the path structure is the same as in the above figure, for example:123456789101112|== means folder; |-- means file. |==resources |==data |--table_design |==mapper |==server |--ServerMapper.xml |==source |--AMapper.xml |--bootstrap.properties |--logback-spring.xml The expected output is:1234567891011depth left_index|||path|||right_index0 1|||\\resources|||201 2|||\\resources\\data||52 3|||\\resources\\data\\table_design|||41 6|||\\resources\\mapper|||152 7|||\\resources\\mapper\\server|||103 8|||\\resources\\mapper\\server\\ServerMapper.xml|||92 11|||\\resources\\mapper\\source|||122 13|||\\resources\\mapper\\AMapper.xml|||141 16|||\\resources\\bootstrap.properties|||171 18|||\\resources\\logback-spring.xml|||19 We can construct the data using the precedence traversal algorithm, traversing its children and assigning values from left to right, one layer at a time.The following code uses a depth-first approach to traverse the directory and assign values, returning Map. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class NestedSetsUtil { /** *Use depth-first traversal of the catalog to return Map, which is used to support nested set initial entry. * * @param root Directories to be traversed * @return Map&lt;File, NestedSetObj&gt; **/ public static Map&lt;File, NestedSetObj&gt; dfs2NestedSets(File root) { if (root == null) { return new HashMap&lt;&gt;(0); } // Record Depth -1 per stack out, +1 per stack in. long depth = 0L; // left and right values long index = 1L; Deque&lt;File&gt; stack = new ArrayDeque&lt;&gt;(); stack.push(root); // A global collection of objects that also indicates whether this object has been on the stack. Map&lt;File, NestedSetObj&gt; map = new LinkedHashMap&lt;&gt;(); // root directory NestedSetObj rootObj = NestedSetObj.builder().path(root.getAbsolutePath()).depth(depth).left(index++).build(); map.put(root, rootObj); while (!stack.isEmpty()) { File cur = stack.pop(); depth--; File[] files = cur.listFiles(); if (files != null) { for (File next : files) { // if the next node hasn't been visited before if (!map.containsKey(next)) { //Stack the current and the next nodes depth++; stack.push(cur); depth++; stack.push(next); // Assign left value to node on first visit NestedSetObj obj = NestedSetObj.builder().left(index++).path(next.getAbsolutePath()).depth(depth).build(); boolean leaf = !next.isDirectory(); boolean emptyDirLeaf = next.listFiles() != null &amp;&amp; next.listFiles().length == 0; //Assign right value to leaf node or empty directory. if (leaf || emptyDirLeaf) { obj.setRight(index++); } map.put(next, obj); break; } } /* * Determine whether the leaf node's right needs to be assigned a value; * The current directory can only be assigned a value if all the data in all the child levels have a RIGHT value. */ long min = Long.MAX_VALUE; for (File file : files) { NestedSetObj childObject = map.get(file); if (childObject == null) { min = 0L; break; } min = Math.min(min, childObject.getRight()); } /* * Assign a value to the right of a non-leaf node. * When the first-level child nodes all have a right value, index+1 is the current node's right value. */ if (min &gt; 0L &amp;&amp; Long.MAX_VALUE != min) { NestedSetObj curObj = map.get(cur); if (curObj.right == 0L) { curObj.setRight(index++); } } } } //Right value of root = nodes * 2 rootObj.setRight(map.size() * 2L); return map; } /** * Document nested set of objects, business-agnostic */ public static class NestedSetObj { private long left; private long right; private long depth; private String path; }} Then just traverse the data from the NestedSetsUtil#dfs2NestedSets() response, assign it to FileNestedSetsDemo, and batch into the library. Query data based on business requirements Retrieve all child nodes of type file under a single path 12345678910SELECT node.*FROM file_nested_sets_demo AS node, file_nested_sets_demo AS parentWHERE node.depth = parent.depth + 1 AND node.left_index &gt; parent.left_index AND node.right_index &lt; parent.right_index AND node.tree_id = 1 AND parent.path = '\\resources\\mapper' AND node.type = 'FILE'ORDER BY parent.left_index Retrieve all child nodes under a single tree 1234SELECT node.*FROM file_nested_sets_demo AS nodeWHERE node.tree_id = 1ORDER BY node.left_index Extended Query Statements Retrieve direct child nodes under a single path 123456789SELECT node.*FROM file_nested_sets_demo AS node, file_nested_sets_demo AS parentWHERE node.depth = parent.depth + 1 AND node.left_index &gt; parent.left_index AND node.right_index &lt; parent.right_index AND node.tree_id = 1 AND parent.path = '\\resources\\mapper'ORDER BY parent.left_index NoteThe Java algorithms that can be executed can be found in GitHub","link":"/implement-SQL-nested-sets-in-Java-en/"}],"tags":[{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","link":"/tags/Kibana/"},{"name":"OPS","slug":"OPS","link":"/tags/OPS/"},{"name":"Cluster","slug":"Cluster","link":"/tags/Cluster/"},{"name":"Deploy","slug":"Deploy","link":"/tags/Deploy/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Tree traversal","slug":"Tree-traversal","link":"/tags/Tree-traversal/"},{"name":"Project Experience","slug":"Project-Experience","link":"/tags/Project-Experience/"},{"name":"System Design","slug":"System-Design","link":"/tags/System-Design/"}],"categories":[{"name":"Technology","slug":"Technology","link":"/categories/Technology/"},{"name":"Backend","slug":"Technology/Backend","link":"/categories/Technology/Backend/"},{"name":"OPS","slug":"Technology/OPS","link":"/categories/Technology/OPS/"},{"name":"System Design","slug":"Technology/System-Design","link":"/categories/Technology/System-Design/"},{"name":"Project Experience","slug":"Technology/Project-Experience","link":"/categories/Technology/Project-Experience/"}],"pages":[{"title":"Comment Policy","text":"简体中文 Welcome to BoboTheKnight’s site! We value open and meaningful discussions and encourage you to leave comments on our technology and lifestyle articles. To ensure a positive and respectful environment for all visitors, we kindly request that you adhere to the following comment policy: Be Respectful: Treat others with kindness, respect, and courtesy. Avoid personal attacks, hate speech, or any form of discrimination. Disagreements are welcome, but express your opinions in a civil and constructive manner. Stay on Topic: Keep your comments relevant to the content of the post or discussion. Avoid going off-topic or posting unrelated promotional content. This helps maintain a focused and meaningful conversation. No Spam: Refrain from using the comment section for self-promotion or advertising purposes. We want to ensure that the comments contribute to the discussion rather than being solely promotional in nature. Avoid Offensive or Inappropriate Content: Ensure that your comments do not contain explicit, obscene, or offensive language or content. Let’s maintain a respectful and family-friendly environment for everyone. Moderation and Removal: The website owner reserves the right to moderate and remove any comments that violate this comment policy. This includes comments that are spam, offensive, off-topic, or otherwise inappropriate. We aim to create a positive and constructive space for everyone involved.Thank you for taking the time to read and understand our comment policy. We appreciate your participation and contribution to BoboTheKnight’s site community. Let’s engage in thoughtful discussions, share insights, and learn from one another while maintaining a respectful and inclusive atmosphere. Enjoy your time here!","link":"/comment-policy/index.html"},{"title":"About","text":"简体中文 The solution of the problem of life is seen in the vanishing of this problem.(Is not this the reason why men to whom after long doubting the sense of life became clear, could not then say wherein this sense consisted?) Ludwig WittgensteinTractatus Logico-Philosophicus BoBoTheKnightWelcome to BoBoTheKnight’s site! I learn and think often out of curiosity. Although the answers to many questions are unspeakable, I still want to share my learning and thinking process.To record my growth. If possible, help more people with the same confusion. Of course, I will also share some technical articles in here.BTW, even I named myself knight, I wish world peace. CreditI want to thank a few people without whom I would have had a hard time building this site.Thanks to GitHub for code hosting!Thanks to Hexo for the blog project!Thanks to Icarus for the pretty theme!Thanks to FontAwesome for the icons!Thanks to FlatIcon for the icons, especially thanks to the following talented authors, they are :&nbsp;&nbsp;&nbsp;&nbsp; Freepik for the Knight and the Peace icons!&nbsp;&nbsp;&nbsp;&nbsp; Trazobanana for the LGBT icons!&nbsp;&nbsp;&nbsp;&nbsp; Rashad for the Peace icons!","link":"/about/index.html"}]}